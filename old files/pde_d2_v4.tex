\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage[a4paper, total={7in, 10in}]{geometry}
\usepackage{subcaption}
\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{example}{Example}[section]
\newtheorem{lemma}{Lemma}[section]
\usepackage{graphicx}
\graphicspath{ {./images/} }

\begin{document}
	\tableofcontents
	\section{Introduction}

	In our paper, we consider a class of quasilinear parabolic partial differential equations where $u$ is defined over $\Omega_T:\Omega\times [0,T]$, where $\Omega\subset\Bbb R^n$ and $T>0$:
	\begin{equation}
	\begin{cases}
 \frac{\partial u}{\partial t} +A(u) =\frac{\partial u}{\partial t}+{\rm div}(a(x,t,u,Du))+a(x,t,u,Du)=0 & \text{for $(x,t) \in \Omega_{T}$}\\
u(0,x) = u_{0}(x), & \text{for $x \in \Omega$} \\
u(t,x) = q(t,x), & \text{ for $x \in \Omega_{T}$} \\
	\end{cases}
	\label{eq:cauchy}
	\end{equation}
	We write the operator of \ref{eq:cauchy} as $f$. The solution $u(x,t)$ can be approximated by multilayered neural networks with respect to the loss function $J$.
Define $Q$ below as the set of all functions implemented by neural networks with one hidden layer, $m$ hidden units and $\textit{l}$ output unit given values at $k$ input units at time t:
	\begin{equation}
	Q(\psi,x,T)=\left \lbrace q(\psi,x_i,t): \mathbb{R}^{\ell+n+1} \rightarrow \mathbb{R}^{\textit{l}}:
q(\psi,x_i,t) = \sum_{j=1}^{m}\Phi_{j}\psi(w_{j}x_{i} + c_j ),\  \ (t,x) \in [0,T]\times \Omega, \ \ \forall i = 0, \dots, n ,  \right\rbrace,
	\label{eq:nn}
	\end{equation}
	where $w\in\mathbb{R^n}$ and $\Phi$ is the inner and output weights,
and $\psi:\mathbb{R}\to\mathbb{R^\ell}$ is the activation function.
The positive integer $k$ is also the number of dimension of PDEs.
	\subsection{Abridged notations}
	\textbf{Quasilinear parabolic PDEs:}
	\begin{enumerate}
			\item $T$ is a finite time interval $[0,T], T>0$
		\item $E_{n}$ is the \textit{n}-dimensional eculidean space; $x = (x_1, \dots , x_n) \forall i = 0, \dots, n$ is an arbitrary points in it.
		\item $E_{n+1}$ is the \textit{n+1}-dimensional eculidean space;
		its points is denoted by $(x,t)$, where $x$ is in $E_{n}$ and $t$ is in $(- \infty, \infty)$
		\item $\Omega$ is a bounded domain in $E_{n}$.
		\item S is the subset of $\Omega$
		\item $\Omega_{T} = H_{T} \cap \Omega$ where $H_{T}$ is the open ball in $E_{n}$ of radius $T$, i.e. the sets of points $(x,t)$ of $E_{n+1}$ with $x \in \Omega$, $t \in (0,T)$
		\item $u(x,t) \in \Omega_{T}$ is the classical solutions of (\ref{eq:cauchy})
		\item $\mu(t)=(\mu_1, \dots, \mu_{n})$ is input measure environment on $\Omega_{T}$.
		\item $\nu(t)$ is a positive continuous function defined for $t \leq 0$
	\end{enumerate}
	\textbf{Neural Network:}
	\begin{enumerate}
		\item Let output units $\textit{l}$ given $k$ input units, so that $\textit{l} \in R^{k}$.
		\item Let $j \in \mathbb{N}^m$ with $j \leq 2$ be the number of hidden units in the m-dimensional hidden layer.
		\item Define the inner weights and output weights as $w \in R^m$ and $\Phi \in R^m$.
		\item Bias $\theta \in \mathbb{R}$ is fixed.
		\item The loss function $J$ measures how well the approximate solutions $q(\psi,x_i,t)$ satisfies $f(x,t,q,Dq)$ in (\ref{eq:cauchy}).
\end{enumerate}
		\subsection{Definition of the function space}
		Let us now introduce the precise setting of (\ref{eq:cauchy}) and (\ref{eq:nn}).
The solution is in the Banach space with the norm
		\begin{equation}
		||u||_{q,\Omega} = \left(\int_{\Omega} |u(x)|^{p} dx\right)^{\frac{1}{p}}
		\end{equation}
		and $u_{0} \in L^{p}(\Omega)$.
In our paper, we aim to implement multi-layer neural networks to approximate the solutions of PDEs in $L^{p}(\mu)$.
$L^{p}(\mu)$ is the Banach space consisting of all measurable functions on $\mu$ that are p-power summable on $\Omega$
		\begin{equation}
		||q(\psi,x_i,t)||_{p,\mu} = \left[\int_{\mu} |q(\psi,x_i,t)|^{p} d\mu(x)\right]^{\frac{1}{p}} < \infty
		\end{equation}
		so that
		\begin{equation}
		J_{p,\mu}(f)  \in L^{p}(\mu),
		\end{equation}
where $A$ is a constant.

		Let the input environment measure $\mu$ be Lebesgue measure on $\Omega$
and $C^{d}(\mathbb{R}^{k})$ denotes the space of all continuous
and differentiable functions $f$ with partial derivative $D^{\alpha}f$ of order $|\alpha| < d$ are continuous on $\mathbb{R}^{k}$.
For $q \in C^{d}(\mathbb{R}^{k})$ and $1 < p < \infty$, f is the usual Sobolev space of order $d$ with the norm.
		\begin{equation}
		||q(\psi,x_i,t)||_{p,\mu} = \left[ \sum_{\alpha \leq d} \int_{\mathbb{R}^{k}} |D^{\alpha}q(\psi,x_i,t)|^{p} d \mu(x)\right]^{\frac{1}{p}}
		\end{equation}



\section{Mathematical analysis of multi-layered neural network}
 The Kolmogorov's superposition theorem presented in 1957 and solved the Hilbert's thirteenth problem \footnotemark about analytic function of three variables that can be expressed as a superposition of bivariate ones.
Kolmogorov superposition theorem found attention in neural network computation by Hecht{-}Nielsen\cite{nielsen}.
The representation of continuous functions defined on an n{-}dimensional cube by sums and superpositions of continuous functions of one variable shows the potential applicability in neural networks\cite{kurkova}.
Due to many scientists devoted to transform Kolmogorov's superposition theorem to numerical algorithm in many years,
Kolmogorov's generalization becomes the current neural network approximate theorem.

\footnotetext{Hilbert considered the seventh-degree equation: $x^{7} + ax^{3} + bx^{2}+cx +1 =0$ and asked whether its solution,  can be the composition of a finite number of two-variable functions.}

\begin{theorem}
	 (\textit{A. Kolmogorov, 1956; V. Arnold, 1957}) Let $f : \mathbb{I}^{n} := [0,1]^{n} \rightarrow\mathbb{R}$ be an arbitrary multivariate continuous function. Then, it has the representation
	 \begin{equation}
	 q(x_1,x_2, \dots, x_n)=\sum_{j=0}^{2m}\Phi_{q}\left(\sum_{i=1}^{n}\phi_{i,j}(x_i)\right)
	 \label{eq:komo}
	 \end{equation}
	 with continuous one-dimensional outer and inner function $\Phi_{j}$ and $\phi_{j,i}$, $0\le j\le 2m$.
Moreover, functions $\phi_{i,j}$ are universal for the given dimension $n$; they are independent of f.
	\end{theorem}
%The original Kolmogorov superposition theorem cannot be used in the algorithm in numerical calculation, because the inner function is highly non-smooth. To make functions $\Phi_{q}$ and $\psi_{q,p}$ in (\ref{eq:komo}) smooth, Kurkova \cite{kurkova}  substituted $\Phi_{q}$ and $\psi_{q,p}$ by  staircase-like functions of any sigmoid type which is continuous with closed interval.
By Theorem 1,2 in \cite{hornik}, we show that neural networks are dense in $C^{d}(\Omega_{T})$.


\begin{lemma}
	If $\psi$ is bounded and nonconstant function, then $q(\psi,x_i,t)$ is dense in $C(\Omega_{T})$ for all compact subsets $X$ of $\mathbb{R}^{k'}$ where $C(\Omega_{T})$ is the space of all continuous functions on $\Omega_{T}$. \label{dense_set}
\end{lemma}
\begin{proof}
	(i) As $q(\psi,x_i,t)$ is bounded, $\{q(\psi,x_i,t)\}$ is a linear subspace of $L^{p}(\mu)$ on $\Omega_{T}$.
If for some $\mu$ and $q(\psi,x_i,t)$ is not dense in $L^{p}(\mu)$, Friedman yields in Corollary 4.8.7 \cite{friedman} that there is a nonzero continuous linear function $\Lambda$ on $L^{p}(\mu)$ that vanishes on $q(\psi,x_i,t)$. Friedman described $\Lambda$ in Corollary 4.14.4 and Theorem 4.14.6 \cite{friedman} that $\Lambda$ is the form
	\[f \rightarrow \Lambda(f)=\int_{\Omega_{T}}fg d\mu\]
	with some $g$ in $L^{p}(\mu)$ where $j$ is the exponent of $j=\frac{i}{i-1}$(For $i=1$ we obtain $j = \infty$; $L^{\infty}(\mu)$ is the space of all functions f for which the $\hat{u}$ the essential supremum
		\[||f|| =\inf\{N>0 : \mu \{x \in \Omega_{T}\} : |q(\psi,x_i,t)| > N\} = 0 \]
		is finite, that is the space of all $\mu$ is bounded functions.)

		Let $\sigma(B) = \int_{B} g d\mu$, we find by H\"older's inequality that for all $B$

		\begin{equation}
		\begin{aligned}
		\left|\sigma(B)\right| &= \left|\int_{\Omega_{T}}1_{B}g d \mu\right|\\
		 & \leq ||1_{B}||_{i,\mu} ||g||_{j,\mu} \leq (\mu(\Omega_{T}))^{\frac{1}{i}}||g||_{j,\mu} < \infty;
		 \end{aligned}
		\end{equation}
	     hence $\mu$ is nonzero finite signed measure on $\mathbb R^{k'}$ such that
	     \begin{equation}
	     \Lambda(f) =\int_{\Omega_{T}}fg d\hat{u} =  \int_{\Omega_{T}}f d\mu
	     \end{equation}
	     As $\Lambda$ vanishes on $Q(\psi,x,T)$, we conclude that
	     \begin{equation}
	     \int_{\Omega_{T}} \psi(wx'+\theta)d \mu(x) = 0
	    \end{equation}
	    for all $w \in\mathbb{R}^{m}$ and $\theta \in \mathbb{R}$, which is impossible
Hence the subspace $q(\psi,x_i,t)$ must be dense in $C(\Omega_{T})$.\\

	    (ii) Suppose that $\psi$ is continuous and that for some compact subset $S$ of $\Omega_{T}$, $q(\psi,x_i,t)$ is not dense in $C(S)$. Proceeding as the proof of Theorem 1 in Cybenko \cite{cybenko}, we find that in this case there exists a nonzero finite signed measure $\mu$ on $\mathbb R^{k'}$ such that
	    \[ \int_{\mathbb{R}^{k'}} \psi(wx'+\theta)d \mu(x) = 0\]
	    for all $w \in\mathbb{R}^{k'}$ and $\theta \in \mathbb{R}$, which is impossible.
Hence the subspace $q(\psi, x, t)$ must be dense in $C(\Omega_{T})$.
\end{proof}
\subsection{Approximation capability of multilayered neural network}


Measuring closeness of functions requires that the activation funcction is nonconstant and derivatives of the approximate function up to order are bounded \cite{hornik}. Then, $q(\psi,x_i,t)$ is dense in weighted Sobolev space $C^{d,p}(\mu)$ which is defined as \cite{hornik}
\begin{equation}
C^{d,p}(\mu) = \{ u \in C^{d}(\Omega_{T}): ||u||_{d,p,\mu} < \infty \}
\end{equation}
Therefore, it guarantees the loss function is smooth functional approximation.

We follow the proof from Theorem 2.2 in \cite{kurkova} and Theorem 7.3 in \cite{sirignano} to prove that the multi-layer neural network can approximate any any function in $L^{p}(\mu)$ if its closeness is measured by $J(f)$.

\begin{theorem}  \label{nn_proof}
	Let $n \in \mathbb{N}$ with $n \geq 2$, $\psi: \Omega_{T} \rightarrow \textit{I}$ be a sigmoid function. $q \in C^{d}(\Omega_{T}))$, and $\epsilon$ be a positive real number. Then, there exist $k \in \mathbb{N}$ and functions $\Phi_{i}$ and $\phi_{i,j} \in Q(\psi,x,T)$ 
	\begin{equation}
	\left|q(x_1,\dots,x_n)-\sum_{j=1}^{2m}\Phi_{j}\left(\sum_{i=1}^{n}\phi_{i,j}(x_p)\right)\right| < \epsilon \text{ for every } (x_1, \dots, x_n) \in I^{n}
	\end{equation}
	such that any continuous function defined on the n-dimension cube can be approximately well by a function belongs to $Q(\psi,x,T)$.
\end{theorem}
\begin{proof}
	By Kolmogorov's superposition theorem,
	\[q(x_1,x_2, \dots, x_n)=\sum_{j=1}^{2m}\Phi_{j}\left(\sum_{i=1}^{n}\phi_{i,j}(x_i)\right).\]
Take $[a,b] \subset \Omega_{T}$ such that for every $i=1,\dots,n$, $j=1, \dots 2m+1$ and $\phi_{j,j}(\textit{I}^{n})\subseteq [a,b]$.
The multilayer neural network is implemented in hidden layers with transfer function $\phi_{i,j}$ from input to the hidden layers and $\Phi_j$ from all of the hidden units to the output one.

By Lemma \ref{dense_set} for every $j = 1, \dots 2m+1$ there exist $g_{j} \in Q(\psi,x,T)$ such that
	\[\left|g_{j}(x)-\Phi_{j}(x)\right|<\frac{\epsilon}{2m(2m+1)} \text{for every } x \in [a,b].\]
	Since $g_{j}$ is uniformly continuous, there exist $\delta$ such that
	\[\left|g_{j}(x)-g_{j}(y)\right| <\frac{\epsilon}{2m(2m+1)} \text{for every } x \in [a,b]
	\ \ \text{with } \left|x-y\right|<\delta \]
	For every $i=1,\dots,n$, $j=1, \dots 2m+1$, there exist $h_{i,j} \in Q(\psi,x,T)$ such that for every $x \in \textit{I}$
	\[\left|h_{i,j}(x)-\phi_{i,j}(x)\right|<\delta \]. Hence, for every $(x_1, \dots , x_n)\in \textit{I}^{n}$
	\[	\left|\sum_{j=1}^{2m+1}g_{j}\left(\sum_{j=1}^{m}h_{i,j}(x_i)\right)-q(x_1,\dots,x_n)\right| < \epsilon\]
\end{proof}


Sirignano \cite{sirignano} presented the theorem that the multilayered feedforward networks is able to universally approximate solutions of quasilinear parabolic PDEs by proving the existence of approximate solutions make the loss function $J(f)$ arbitrarily small.

\begin{theorem}\cite{sirignano}
	Let the $L^{2}$ error $J(f)$ measures how well the approximate solution $f$ satisfies the differential operator in the equation.

	\noindent \medspace
	Let $f$ be a neural network which minimizes loss function $J(f)$. There exists
	\[f_i \in \textit{Q} \textrm{ such that } J(f) \rightarrow 0 \textrm{ as } n \rightarrow \infty \text{ and}\]
	\[ f_{i} \rightarrow u  \text{ as } n \rightarrow \infty.\] Therefore, using the results from the proof of Theorem 3 \cite{hornik}, for all $u \in C^{d}(\Omega_{T}))$ and $\epsilon > 0$, there is a function $f_{i}$ such that
	\begin{equation}\label{eq:m_dense}
	||q(\psi, x_{i},t)-A||_{m,p,\mu} < \epsilon
	\end{equation}


\end{theorem}

\begin{proof}
	Hornik in Theorem 3 \cite{hornik} presented that if the activation function $\psi \in C^{d}(\Omega_{T})$ is nonconstant and bounded, $q(\psi,x_i,t) \in Q(\psi,x,T)$ is uniformly m-dense on the compacts of $C^{d}(\mathbb{R}^{k})$ and dense $C^{m,p}(\hat{u})$ In our proof, we consider the PDEs is second order,so $d=2$.

	%We aim to find the approximate solutions in $C^{2,p}(\hat{u})$.
	We assume that $(u,\bigtriangledown_{x}u) \rightarrow q(t,x,u,\bigtriangledown_{x}u)$ is locally Lipschitz continuous\footnotemark  in $(u,\bigtriangledown_{x}u)$ with Lipschiz constant. Let $f$ be the PDE function we aim to solve. This means that
	\footnotetext{Suppose $A \subset \mathbb{R}^{n}$ is open and $f : A \rightarrow \mathbb{R}^{n}$ is differentiable. A function is locally Lipschitz continuous if there exists a contant $K>0$ and $\delta > 0$such that $|x_1-x_2|>\delta$ implies $|q(x_1)-q(x_2)| \leq K |x_1-x_2|^{}$.}

	\begin{equation}\label{eq:lip}
	\left|q(t,x,u,\bigtriangledown_{x}u) - q(t,x,\hat{u},\bigtriangledown_{x}\hat{u})\right| \leq \left(|u|^{q_{1}/2} + |\bigtriangledown_{x}u|^{q_{2}/2} + |\hat{u}|^{q_{3}/2} + |\bigtriangledown_{x}\hat{u}|^{q_{4}/2}\right)\left(|u-\hat{u}| + |\bigtriangledown_{x}u-\bigtriangledown_{x}\hat{u}|\right)
	\end{equation}
	for some constants $0<q_{1},q_{2},q_{3},q_{4} < \infty$. We integrate (\ref{eq:lip}) and using Holder inequality with exponents $r_{1}$, $r_{2}$:
	\begin{equation}
	\begin{aligned}
		&\int_{\Omega_{T}}\left| \gamma(t,x,u,\bigtriangledown_{x}u) - \hat{\gamma}(t,x,\hat{u},\bigtriangledown_{x}\hat{u})\right|^{2} dv_{1}(t,x) \leq \\
	&\int_{\Omega_{T}}\left(|u|^{q_{1}} + |\bigtriangledown_{x}u|^{q_{2}} + |\hat{u}|^{q_{3}} + |\bigtriangledown_{x}\hat{u}|^{q_{4}}\right)
	\left(|u-\hat{u}|^{2} + |\bigtriangledown_{x}u-\bigtriangledown_{x}\hat{u}|^{2}\right)dv_{1}(t,x) \leq\\
	&\left(\int_{\Omega_{T}}\left(|u|^{q_{1}} + |\bigtriangledown_{x}u|^{q_{2}} + |\hat{u}|^{q_{3}} + |\bigtriangledown_{x}\hat{u}|^{q_{4}}\right)^{r_{1}}dv_{1}(t,x)\right)^{1/r_{1}}
	\left(\int_{\Omega_{T}}\left(|u-\hat{u}|^{2} + |\bigtriangledown_{x}u-\bigtriangledown_{x}\hat{u}|^{2}\right)^{r_{2}}dv_{1}(t,x)\right)^{1/r_{2}} \leq\\
	K&\left(\int_{\Omega_{T}}\left(|u|^{q_{1}} + |\bigtriangledown_{x}u|^{q_{2}} + |\hat{u}|^{\max \{q_{1}q_{3}\}} + |\bigtriangledown_{x}\hat{u}|^{\max \{q_{2},q_{4}\}}\right)^{r_{1}}dv_{1}(t,x)\right)^{1/r_{1}}\\
	&\times\left(\int_{\Omega_{T}}\left(|u-\hat{u}|^{2} + |\bigtriangledown_{x}u-\bigtriangledown_{x}\hat{u}|^{2}\right)^{r_{2}}dv_{1}(t,x)\right)^{1/r_{2}} \leq
	K(\epsilon^{q_{1}}+\epsilon^{q_{2}}+sup|u|^{\max \{q_{1}q_{3}\}} + sup|\bigtriangledown_{x}u|^{\max \{q_{2}q_{4}\}} )\epsilon^{2}
	\end{aligned}
	\end{equation}
	The last inequality is from (\ref{eq:m_dense})

\end{proof}

\section{Experiments and Results}

We use tensorflow to implement our multilayered model. The algorithm is tested on a class of high-dimensional PDEs. The results shows that the solution calculated from our model satisfies the $f$ in nonlinear PDEs (\ref{eq:cauchy}) to zero. 

\begin{figure}
	\includegraphics[width=0.5\linewidth]{ode_d1.jpg}
	\caption{First order ODE. The loss is the value of f in (\ref{eq:cauchy}). The value of loss function approaches to zero as the number of iteration of our model increase.}
	\label{fig:ode_d1}
\end{figure}

\begin{figure}
	\includegraphics[width=0.5\linewidth]{ode_d2.jpg}
	\caption{Second order ODE. The loss is the value of f in (\ref{eq:cauchy}). The value of loss function approaches to zero as the number of iteration of our model increase.}
	\label{fig:ode_d2}
\end{figure}


\section{Literature Review}

Using deep neural network to find the solutions for PDEs has rising in the computational mathematics field in these years.  Finite difference methods become infeasible in higher dimensions due to the explosion in the number of grid points and the demand for reduced time step size. Sirignano \cite{sirignano} provided the proof for the approximation of PDE solutions with neural networks. We will present the proof in (Section 2). We propose to solve high-dimensional PDEs using a deep learning algorithm which uses multi-layer neural networks. A multi-layer neural network is composed with nonlinear operations with parameters estimating from data.


\bibliographystyle{unsrt}
\bibliography{ref}







\end{document}
